{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kmeans.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 2 2 1 0 2 2 2 0 1 2 1 0 0 1 0 2 0 2 0 1 2 2 1 2 1 1 0 0 2 2 2 0 2 2\n",
      " 0 2 0 2 1 0 1 2 2 0 2 2 2 0 0 2 1 1 0 0 2 2 0 1 0 2 1 1 1 1 2 1 0 0 1 0 1\n",
      " 0 1 0 1 0 0 0 0 0 0 0 0 2 1 1 1 0 0 1 0 0 2 1 2 1 1 0 0 2 0 1 2 1 0 0 2 1\n",
      " 2 2 1 2 0 0 2 2 0 1 1 0 1 0 0 0 2 1 0 1 1 1 2 0 2 0 2 0 0 0 2 0 0 2 1 0 2\n",
      " 1 2 2 2 0 2 2 1 2 1 0 2 1 2 2 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 2 1 0 1 2\n",
      " 0 2 2 2 0 1 0 1 2 0 1 0 1 1 0 2 2 2 2 1 1 2 2 1 0 1 1 2 0 1 0 1 1 2 0 1 2\n",
      " 0 0 0 0 0 1 2 1 1 2 2 0 2 1 0 1 1 2 1 1 2 1 2 0 0 0 2 0 2 2 0 2 1 0 2 1 1\n",
      " 0 1 0 1 2 2 2 1 1 0 0 2 0 2 0 0 1 2 0 0 0 1 2 0 1 2 0 0 1 1 0 0 0 1 2 2 0\n",
      " 2 2 0 2 0 0 2 0 2 0 2 0 1 0 2 0 2 0 1 0 1 2 1 0 0 2 1 1 0 2 2 0 2 2 2 1 2\n",
      " 2 0 1 0 1 0 0 2 1 1 1 2 1 2 1 0 1 0 0 2 0 2 1 0 2 2 2 2 2 2 1 2 1 0 0 1 0\n",
      " 2 0 0 1 1 1 1 0 0 1 2 2 1 0 1 0 1 0 2 1 2 1 0 1 0 2 0 0 1 1 0 2 2 2 2 2 0\n",
      " 1 0 2 1 1 2 2 0 2 2 2 2 2 1 0 1 2 2 0 1 2 1 1 0 1 2 0 0 2 0 2 2 1 1 1 0 0\n",
      " 1 0 2 1 0 1 2 1 1 0 1 1 1 0 1 1 1 0 2 1 0 0 1 1 1 1 0 2 1 2 0 1 2 0 0 1 2\n",
      " 2 2 1 0 2 0 1 0 2 0 2 2 1 2 2 0 0 2 0 0 1 2 0 1 1 0 2 0 1 2 2 1 1 0 0 2 0\n",
      " 0 1 1 2 0 0 1 2 2 2 2 0 2 0 2 1 2 0 2 0 2 0 1 2 1 2 2 0 0 2 0 1 2 2 2 2 0\n",
      " 1 0 1 1 1 2 1 2 2 2 2 0 0 2 0 0 1 1 2 0 0 0 0 1 0 0 1 0 1 0 1 2 1 0 1 2 2\n",
      " 2 0 0 2 0 0 2 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 1 0 0 0 2 1 1 1 2 0 2 2 1 2 0\n",
      " 2 2 2 0 1 0 0 1 2 1 2 1 2 0 2 1 0 1 1 1 2 0 2 2 0 1 0 2 1 1 1 2 0 0 1 1 1\n",
      " 1 2 2 0 2 1 0 1 0 2 1 1 1 1 1 1 1 2 0 0 2 2 0 0 1 0 1 2 0 2 0 1 0 1 2 0 0\n",
      " 2 1 2 2 2 1 1 2 0 0 2 1 1 1 1 0 2 1 0 1 2 2 1 0 2 1 0 1 1 2 1 1 2 2 2 1 2\n",
      " 0 2 0 1 1 2 1 0 0 2 0 1 2 1 2 1 2 2 2 1 1 2 1 2 2 2 1 1 2 2 0 2 0 0 0 1 0\n",
      " 0 0 0 2 0 2 0 1 1 1 0 2 0 1 0 2 0 2 2 0 0 1 1 2 1 0 1 1 1 2 0 2 2 1 1 1 2\n",
      " 0 1 0 2 1 1 1 1 2 0 1 0 0 0 1 2 1 2 0 2 1 2 2 0 2 1 2 1 1 2 2 2 1 1 1 1 1\n",
      " 2 2 2 0 2 2 1 0 1 0 0 0 2 0 0 0 0 0 2 1 0 2 0 1 0 1 1 2 0 2 0 1 2 0 1 1 2\n",
      " 0 2 0 2 0 2 2 0 0 2 0 1 2 0 2 0 2 1 2 2 0 2 2 1 1 1 1 1 2 2 2 1 1 2 1 2 2\n",
      " 1 0 0 2 1 0 0 1 2 2 1 2 2 2 0 0 2 2 1 2 1 1 1 1 2 0 1 0 1 2 1 0 1 1 2 2 0\n",
      " 2 0 0 0 1 0 1 1 2 1 0 0 2 0 0 1 2 2 0 1 2 2 2 0 1 0 0 2 2 0 0 1 0 0 2 1 1\n",
      " 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Using SKLearns API for performing Kmeans clustering.\n",
    "Using sklearn.datasets.make_blobs for generating randomized gaussians\n",
    "for clustering.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np \n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs \n",
    "\n",
    "# create a dataset sample space that will be used\n",
    "# to test KMeans. Use function : make_blobs\n",
    "# \n",
    "\n",
    "n_samples = 1000\n",
    "n_features = 5;\n",
    "n_clusters = 3;\n",
    "\n",
    "# aint this sweet \n",
    "X, y = make_blobs(n_samples, n_features) \n",
    "# X => array of shape [nsamples,nfeatures] ;;; y => array of shape[nsamples]\n",
    "\n",
    "# X : generated samples, y : integer labels for cluster membership of each sample\n",
    "# \n",
    "# \n",
    "\n",
    "# performing KMeans clustering\n",
    "\n",
    "ret =  KMeans(n_clusters = n_clusters).fit_predict(X)\n",
    "print(ret)\n",
    "\n",
    "__, ax = plt.subplots(2)\n",
    "ax[0].scatter(X[:,0], X[:,1])\n",
    "ax[0].set_title(\"Initial Scatter Distribution\")\n",
    "ax[1].scatter(X[:,0], X[:,1], c=ret)\n",
    "ax[1].set_title(\"Colored Partition denoting Clusters\")\n",
    "# plt.scatter\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "from glob import glob \n",
    "import argparse\n",
    "#from helpers import *\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "\n",
    "class BOV:\n",
    "    def __init__(self, no_clusters):\n",
    "        self.no_clusters = no_clusters\n",
    "        self.train_path = None\n",
    "        self.test_path = None\n",
    "        self.im_helper = ImageHelpers()\n",
    "        self.bov_helper = BOVHelpers(no_clusters)\n",
    "        self.file_helper = FileHelpers()\n",
    "        self.imageCount = 0\n",
    "        self.trainImageCount = 0\n",
    "        self.train_labels = np.array([])\n",
    "        self.name_dict = {}\n",
    "        self.descriptor_list = []\n",
    "        \n",
    "        self.path = None\n",
    "        self.gray = 0\n",
    "        \n",
    "        self.train_images = []\n",
    "        self.train_img_names = []\n",
    "        self.train_cls = []\n",
    "        \n",
    "        self.test_images = []\n",
    "        self.test_img_names = []\n",
    "        self.test_cls = []      \n",
    "\n",
    "        \n",
    "    def dictionaryfy(self):\n",
    "        train_dict = {}\n",
    "        for i, image in enumerate(self.train_images):        \n",
    "            cls = str(self.train_cls[i]).split('_')[0]\n",
    "            if (cls not in train_dict):\n",
    "                train_dict[cls] = []\n",
    "\n",
    "            train_dict[cls].append(image)\n",
    "        self.images = train_dict\n",
    "        \n",
    "        test_dict = {}\n",
    "        for i, image in enumerate(self.test_images):        \n",
    "            cls = str(self.test_cls[i]).split('_')[0]\n",
    "            if (cls not in test_dict):\n",
    "                test_dict[cls] = []\n",
    "\n",
    "            test_dict[cls].append(image)\n",
    "        self.testImages = test_dict\n",
    "        \n",
    "     \n",
    "    def extractFeatures(self):\n",
    "        # extract SIFT Features from each image\n",
    "        print(\"train image count: \", self.trainImageCount)\n",
    "        label_count = 0 \n",
    "        for word, imlist in self.images.items():\n",
    "            self.name_dict[str(label_count)] = word\n",
    "            print(\"Computing Features for \", word)\n",
    "            #print(\"List: \", imlist)\n",
    "            for im in imlist:\n",
    "                # cv2.imshow(\"im\", im)\n",
    "                # cv2.waitKey()\n",
    "                self.train_labels = np.append(self.train_labels, label_count)\n",
    "                #plt.figure()\n",
    "                #plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "                \n",
    "                kp, des = self.im_helper.features(im) #self.im_helper.gray(im)\n",
    "                #featureV = self.im_helper.featuresPL(im)\n",
    "                #print(\"featureV: \", featureV)\n",
    "                #print(\"des: \", des)\n",
    "                \n",
    "                \n",
    "                \n",
    "                self.descriptor_list.append(des)\n",
    "\n",
    "            label_count += 1\n",
    "            \n",
    "        #print(self.descriptor_list)\n",
    "    \n",
    "    def trainModel(self):\n",
    "        \"\"\"\n",
    "        This method contains the entire module \n",
    "        required for training the bag of visual words model\n",
    "\n",
    "        Use of helper functions will be extensive.\n",
    "\n",
    "        \"\"\"\n",
    "        # perform clustering   \n",
    "        bov_descriptor_stack = self.bov_helper.formatND(self.descriptor_list)\n",
    "        self.bov_helper.cluster()\n",
    "        self.bov_helper.developVocabulary(n_images = self.trainImageCount, descriptor_list=self.descriptor_list)\n",
    "\n",
    "        # show vocabulary trained\n",
    "        self.bov_helper.plotHist()\n",
    " \n",
    "\n",
    "        self.bov_helper.standardize()\n",
    "        self.bov_helper.train(self.train_labels)\n",
    "        print(\"trainModel DONE.\")\n",
    "\n",
    "\n",
    "    def recognize(self,test_img, test_image_path=None):\n",
    "\n",
    "        \"\"\" \n",
    "        This method recognizes a single image \n",
    "        It can be utilized individually as well.\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        kp, des = self.im_helper.features(test_img)\n",
    "        #des = self.im_helper.featuresPL(im)\n",
    "        # print kp\n",
    "        print(des.shape)\n",
    "\n",
    "        # generate vocab for test image\n",
    "        vocab = np.array( [[ 0 for i in range(self.no_clusters)]])\n",
    "        # locate nearest clusters for each of \n",
    "        # the visual word (feature) present in the image\n",
    "        \n",
    "        # test_ret =<> return of kmeans nearest clusters for N features\n",
    "        test_ret = self.bov_helper.kmeans_obj.predict(des)\n",
    "        # print test_ret\n",
    "\n",
    "        # print vocab\n",
    "        for each in test_ret:\n",
    "            vocab[0][each] += 1\n",
    "\n",
    "        print(vocab)\n",
    "        # Scale the features\n",
    "        vocab = self.bov_helper.scale.transform(vocab)\n",
    "\n",
    "        # predict the class of the image\n",
    "        lb = self.bov_helper.clf.predict(vocab)\n",
    "        # print \"Image belongs to class : \", self.name_dict[str(int(lb[0]))]\n",
    "        return lb\n",
    "\n",
    "\n",
    "\n",
    "    def testModel(self):\n",
    "        \"\"\" \n",
    "        This method is to test the trained classifier\n",
    "\n",
    "        read all images from testing path \n",
    "        use BOVHelpers.predict() function to obtain classes of each image\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        #self.testImages, self.testImageCount = self.file_helper.getFiles(self.test_path)\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        for word, imlist in self.testImages.items():\n",
    "            print(\"processing \" ,word)\n",
    "            for im in imlist:\n",
    "                # print imlist[0].shape, imlist[1].shape\n",
    "                print(im.shape)\n",
    "                cl = self.recognize(im)\n",
    "                print(cl)\n",
    "                predictions.append({\n",
    "                    'image':im,\n",
    "                    'class':cl,\n",
    "                    'object_name':self.name_dict[str(int(cl[0]))]\n",
    "                    })\n",
    "\n",
    "        print(predictions)\n",
    "        for each in predictions:\n",
    "            # cv2.imshow(each['object_name'], each['image'])\n",
    "            # cv2.waitKey()\n",
    "            # cv2.destroyWindow(each['object_name'])\n",
    "            # \n",
    "            #if (self.gray):\n",
    "            #    plt.imshow(cv2.cvtColor(each['image'], cv2.COLOR_GRAY2RGB))\n",
    "            #else: \n",
    "            #    plt.imshow(cv2.cvtColor(each['image'], cv2.COLOR_BGR2RGB))\n",
    "            plt.imshow(Image.fromarray(each['image']))\n",
    "            plt.title(each['object_name'])\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    def print_vars(self):\n",
    "        pass\n",
    "    \n",
    "    def loadFruits(self, path):\n",
    "        self.path = path\n",
    "        \n",
    "        # read file. prepare file lists.\n",
    "        self.train_images, self.train_img_names = self.file_helper.get_fruit_images(self.path)\n",
    "        self.train_cls = [name.split('_')[0] for name in self.train_img_names]\n",
    "        \n",
    "        # train the model\n",
    "        #self.trainModel()\n",
    "        # test model\n",
    "        #self.testModel()\n",
    "    \n",
    "    def trainTestSplit(self):\n",
    "        self.train_images, self.test_images, self.train_cls, self.test_cls = train_test_split(self.train_images, self.train_cls, test_size=0.2, random_state=9253, stratify=self.train_cls)\n",
    "        self.trainImageCount = len(self.train_images)\n",
    "        self.testImageCount = len(self.test_images)\n",
    "    \n",
    "    def loadCars(self, path):\n",
    "        self.path = path\n",
    "        \n",
    "        self.train_images, self.train_img_names = self.file_helper.get_car_images(self.path)\n",
    "        self.train_cls = [0 if name.startswith('neg') else 1 for name in self.train_img_names]\n",
    "        self.trainImageCount = len(self.train_images)\n",
    "        \n",
    "        \n",
    "        self.test_images, self.test_img_names = self.file_helper.get_car_images(self.path, 'TestImages')\n",
    "        self.test_cls = [0 if name.startswith('neg') else 1 for name in self.test_img_names]\n",
    "        self.testImageCount = len(self.test_images)\n",
    "        \n",
    "        # set testing paths\n",
    "        #self.test_path = test_path\n",
    "        # train the model\n",
    "        #self.trainModel()\n",
    "        # test model\n",
    "        #self.testModel()    \n",
    "\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     # parse cmd args\n",
    "#     parser = argparse.ArgumentParser(\n",
    "#             description=\" Bag of visual words example\"\n",
    "#         )\n",
    "#     parser.add_argument('--train_path', action=\"store\", dest=\"train_path\", required=True)\n",
    "#     parser.add_argument('--test_path', action=\"store\", dest=\"test_path\", required=True)\n",
    "\n",
    "#     args =  vars(parser.parse_args())\n",
    "#     print(args)\n",
    "\n",
    "    \n",
    "#     bov = BOV(no_clusters=100)\n",
    "\n",
    "#     # set training paths\n",
    "#     bov.train_path = args['train_path'] \n",
    "#     # set testing paths\n",
    "#     bov.test_path = args['test_path'] \n",
    "#     # train the model\n",
    "#     bov.trainModel()\n",
    "#     # test model\n",
    "#     bov.testModel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "import tarfile\n",
    "import io\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import gc\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "class ImageHelpers:\n",
    "    def __init__(self):\n",
    "        self.sift_object = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "    def gray(self, image):\n",
    "        plt.figure()\n",
    "        plt.imshow(image)\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        return gray\n",
    "\n",
    "    def colorRGB(self, image):\n",
    "        colorRGB = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        return colorRGB\n",
    "    \n",
    "    def features(self, image):\n",
    "        keypoints, descriptors = self.sift_object.detectAndCompute(image, None)\n",
    "        return [keypoints, descriptors]\n",
    "    \n",
    "    def featuresPL(self, image):\n",
    "        im = Image.fromarray(image)\n",
    "        featureVector = im.histogram()\n",
    "        return featureVector\n",
    "\n",
    "\n",
    "class BOVHelpers:\n",
    "    def __init__(self, n_clusters = 20):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.kmeans_obj = KMeans(n_clusters = n_clusters)\n",
    "        self.kmeans_ret = None\n",
    "        self.descriptor_vstack = None\n",
    "        self.mega_histogram = None\n",
    "        self.clf  = SVC()    \n",
    "\n",
    "    def cluster(self):\n",
    "        \"\"\"    \n",
    "        cluster using KMeans algorithm, \n",
    "\n",
    "        \"\"\"\n",
    "        self.kmeans_ret = self.kmeans_obj.fit_predict(self.descriptor_vstack)\n",
    "\n",
    "    def developVocabulary(self,n_images, descriptor_list, kmeans_ret = None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Each cluster denotes a particular visual word \n",
    "        Every image can be represeted as a combination of multiple \n",
    "        visual words. The best method is to generate a sparse histogram\n",
    "        that contains the frequency of occurence of each visual word \n",
    "\n",
    "        Thus the vocabulary comprises of a set of histograms of encompassing\n",
    "        all descriptions for all images\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.mega_histogram = np.array([np.zeros(self.n_clusters) for i in range(n_images)])\n",
    "        old_count = 0\n",
    "        print(\"n_images:\",n_images)\n",
    "        print(\"descriptor_list length:\", len(descriptor_list))\n",
    "        \n",
    "        for i in range(n_images):\n",
    "            #print(i)\n",
    "            l = len(descriptor_list[i])\n",
    "            for j in range(l): \n",
    "                #print(i, j)\n",
    "                if kmeans_ret is None:\n",
    "                    #if (old_count+j==n_images):\n",
    "                    #    print(old_count+j)\n",
    "                    #    break;\n",
    "                    #else: \n",
    "                    idx = self.kmeans_ret[old_count+j]\n",
    "                else:\n",
    "                    idx = kmeans_ret[old_count+j]\n",
    "                #print(\"why are we still here\")\n",
    "                self.mega_histogram[i][idx] += 1\n",
    "            old_count += l\n",
    "        print(\"Vocabulary Histogram Generated\")\n",
    "\n",
    "    def standardize(self, std=None):\n",
    "        \"\"\"\n",
    "        \n",
    "        standardize is required to normalize the distribution\n",
    "        wrt sample size and features. If not normalized, the classifier may become\n",
    "        biased due to steep variances.\n",
    "\n",
    "        \"\"\"\n",
    "        if std is None:\n",
    "            self.scale = StandardScaler().fit(self.mega_histogram)\n",
    "            self.mega_histogram = self.scale.transform(self.mega_histogram)\n",
    "        else:\n",
    "            print(\"STD not none. External STD supplied\")\n",
    "            self.mega_histogram = std.transform(self.mega_histogram)\n",
    "\n",
    "    def formatND(self, l):\n",
    "        \"\"\"    \n",
    "        restructures list into vstack array of shape\n",
    "        M samples x N features for sklearn\n",
    "\n",
    "        \"\"\"\n",
    "        #print(\"l: \")\n",
    "        #print(l)\n",
    "        \n",
    "        vStack = np.array(l[0])\n",
    "        for remaining in l[1:]:\n",
    "            vStack = np.vstack((vStack, remaining))\n",
    "        self.descriptor_vstack = vStack.copy()\n",
    "        return vStack\n",
    "\n",
    "    def train(self, train_labels):\n",
    "        \"\"\"\n",
    "        uses sklearn.svm.SVC classifier (SVM) \n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        print(\"Training SVM\")\n",
    "        print(self.clf)\n",
    "        print(\"Train labels\", train_labels)\n",
    "        self.clf.fit(self.mega_histogram, train_labels)\n",
    "        print(\"Training completed\")\n",
    "\n",
    "    def predict(self, iplist):\n",
    "        predictions = self.clf.predict(iplist)\n",
    "        return predictions\n",
    "\n",
    "    def plotHist(self, vocabulary = None):\n",
    "        print(\"Plotting histogram\")\n",
    "        if vocabulary is None:\n",
    "            vocabulary = self.mega_histogram\n",
    "\n",
    "        x_scalar = np.arange(self.n_clusters)\n",
    "        y_scalar = np.array([abs(np.sum(vocabulary[:,h], dtype=np.int32)) for h in range(self.n_clusters)])\n",
    "\n",
    "        print(y_scalar)\n",
    "\n",
    "        plt.bar(x_scalar, y_scalar)\n",
    "        plt.xlabel(\"Visual Word Index\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.title(\"Complete Vocabulary Generated\")\n",
    "        plt.xticks(x_scalar + 0.4, x_scalar)\n",
    "        plt.show()\n",
    "\n",
    "class FileHelpers:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def resize_images(images):\n",
    "        \"\"\"\n",
    "        used to resize all images to the mean height/width\n",
    "        note that this is prone to generating distorted images\n",
    "        better results are expected with methods like pyramid pooling (https://arxiv.org/abs/1406.4729)\n",
    "        \"\"\"\n",
    "        target_size = 200\n",
    "        mean_shape = np.mean([[i.shape[0],i.shape[1]] for i in images], axis=0)\n",
    "        max_v = max(mean_shape[0], mean_shape[1])\n",
    "        # scale by preserving average ratio\n",
    "        target_height = int(target_size * mean_shape[0]/max_v)\n",
    "        target_width = int(target_size * mean_shape[1]/max_v)\n",
    "        for i in range(len(images)):\n",
    "            images[i] = np.array(Image.fromarray(images[i]).resize((target_width, target_height), Image.ANTIALIAS))\n",
    "            print(\"Resized {} fruit images\".format(i), end=\"\\r\")\n",
    "\n",
    "        return images\n",
    "    \n",
    "    def get_car_images(self, car_data_path, folder = 'TrainImages'):\n",
    "        print(car_data_path, folder)\n",
    "        p = os.path.join(car_data_path,folder,'*.pgm')\n",
    "        files = glob.glob(p)\n",
    "        print(\"Found {} car files\".format(len(files)))\n",
    "        images = []\n",
    "        image_names = []\n",
    "\n",
    "        for filename in files:\n",
    "            image_names.append(os.path.basename(filename))\n",
    "            with Image.open(filename) as img:\n",
    "                images.append(np.array(img))\n",
    "\n",
    "        return np.array(images), image_names\n",
    "\n",
    "    def get_fruit_images(self, fruits_data_path):\n",
    "        images = []\n",
    "        image_names = []\n",
    "        cnt=0\n",
    "\n",
    "        for dirpath, dirnames, filenames in os.walk(fruits_data_path):\n",
    "            for filename in [f for f in filenames if f.endswith(\".jpg\")]:\n",
    "                cnt = cnt+1\n",
    "                with Image.open(os.path.join(dirpath,filename)) as img:\n",
    "                    if img.format != \"JPEG\":\n",
    "                        # we only want 3 channels\n",
    "                        img = img.convert(\"RGB\")\n",
    "                    np_img = np.array(img)\n",
    "                    if len(np_img.shape) != 3 or np_img.shape[2] != 3:\n",
    "                        # incorrectly imported/converted RGB file\n",
    "                        continue\n",
    "                    images.append(np_img)\n",
    "                    image_names.append(os.path.split(dirpath)[-1] + \"_\" + filename)\n",
    "                    print(\"Found {} fruit files\".format(len(image_names)), end=\"\\r\")\n",
    "\n",
    "        #images = np.stack(images, axis=0)\n",
    "\n",
    "        return images, image_names\n",
    "    \n",
    "    def getFiles(self, path):\n",
    "        \"\"\"\n",
    "        - returns  a dictionary of all files \n",
    "        having key => value as  objectname => image path\n",
    "\n",
    "        - returns total number of files.\n",
    "\n",
    "        \"\"\"\n",
    "        imlist = {}\n",
    "        count = 0\n",
    "        for each in glob(path + \"*\"):\n",
    "            each = each.replace(\"\\\\\", \"/\")\n",
    "            word = each.split(\"/\")[-1]\n",
    "            print(\" #### Reading image category \", word, \" ##### \")\n",
    "            imlist[word] = []\n",
    "            for imagefile in glob(path+word+\"/*\"):\n",
    "                imagefile = imagefile.replace(\"\\\\\", \"/\")\n",
    "                print(\"Reading file \", imagefile)\n",
    "                \n",
    "                \n",
    "                im = Image.open(imagefile)\n",
    "                if (im.format != \"JPEG\"):\n",
    "                    im = im.convert(\"RGB\")\n",
    "                np_im = np.array(im)\n",
    "                if len(np_im.shape) != 3 or np_im.shape[2] != 3:\n",
    "                    # incorrectly imported/converted RGB file\n",
    "                    continue\n",
    "                \n",
    "                #im = cv2.imread(imagefile,1)\n",
    "                \n",
    "                #plt.imshow(im)\n",
    "                imlist[word].append(np_im)\n",
    "                count +=1 \n",
    "\n",
    "        return [imlist, count]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../ex3/CarData/ TrainImages\n",
      "Found 133 car files\n",
      "../../../ex3/CarData/ TestImages\n",
      "Found 170 car files\n",
      "Found 812 fruit files\r"
     ]
    }
   ],
   "source": [
    "cars = BOV(no_clusters=100)\n",
    "cars.loadCars(\"../../../ex3/CarData/\")\n",
    "fruits = BOV(no_clusters=100)\n",
    "fruits.loadFruits(\"../../ex3/FIDS30/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.fromarray(fruits.train_images[0])\n",
    "plt.imshow(im)\n",
    "plt.title(fruits.train_img_names[0])\n",
    "\n",
    "featureVector = im.histogram()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(featureVector[:256], 'r')\n",
    "plt.plot(featureVector[257:512], 'g')\n",
    "plt.plot(featureVector[513:], 'b')\n",
    "plt.xlim([0, 256])\n",
    "plt.xlabel(\"Bins\")\n",
    "plt.ylabel(\"# of Pixels\")\n",
    "plt.title(\"Colour Histogram, using PIL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{} unique classes for cars\".format(len(set(cars.train_cls))))\n",
    "print(\"{} unique classes for fruits\".format(len(set(fruits.train_cls))))\n",
    "\n",
    "fruits.trainTestSplit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cars.dictionaryfy()\n",
    "fruits.dictionaryfy()\n",
    "im = Image.fromarray(fruits.images['watermelons'][0])\n",
    "plt.imshow(im)\n",
    "plt.title('watermelons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time cars.extractFeatures()\n",
    "#cars.testModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%time cars.trainModel()\n",
    "# perform clustering   \n",
    "#%time bov_descriptor_stack = cars.bov_helper.formatND(cars.descriptor_list)\n",
    "#%time cars.bov_helper.cluster()\n",
    "#%time cars.bov_helper.developVocabulary(n_images = cars.trainImageCount, descriptor_list=cars.descriptor_list)\n",
    "\n",
    "# show vocabulary trained\n",
    "#%time cars.bov_helper.plotHist()\n",
    "\n",
    "\n",
    "#%time cars.bov_helper.standardize()\n",
    "#%time cars.bov_helper.train(cars.train_labels)\n",
    "#print(\"trainModel DONE.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,l in enumerate(cars.descriptor_list):\n",
    "    print(i)\n",
    "    if (l is None):\n",
    "        print(i)\n",
    "        print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time fruits.extractFeatures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#bov = BOV(no_clusters=100)\n",
    "#bov.start(\"images\\\\train\\\\\", \"images\\\\test\\\\\")\n",
    "#bov.start(\"../../../ex3/CarData/TrainImages/\", \"../../../ex3/CarData/TestImages/\")\n",
    "#bov.start(\"../../../ex3/FIDS30/train/\", \"../../../ex3/FIDS30/test/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
